{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c96442-7f94-45c1-b467-b1417869c3f2",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc392b5-4a5e-41d4-ad2e-09b1358b03bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pdb, os\n",
    "import yaml\n",
    "import math\n",
    "import joblib\n",
    "from utils.dataset import CovarianceTemporalDataset, CovarianceLSTMDataset\n",
    "from utils.tools import back_to_matcov\n",
    "from utils.models import GATModel, RecurrentGCN\n",
    "from utils.dataset import CovarianceTemporalDataset,CovarianceLaggedDataset,CovarianceSparseDataset,CovarianceLaggedMultiOutputDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from glob import glob\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "from linearmodels import PanelOLS\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac4c3e-a98c-417e-9564-17761cd21f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + 'YOURPATH'\n",
    "\n",
    "# 'YOURPATH' could like like 'C:\\\\Users\\\\USERNAME\\\\AppData\\\\Local\\\\Programs\\\\MiKTeX\\\\miktex\\\\bin\\\\x64' on Windows\n",
    "\n",
    "columnwidth = 418.25 \n",
    "style = \"white\"  # darkgrid\n",
    "fsize = 14\n",
    "params = {\n",
    "    \"text.usetex\": True,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.family\": \"serif\",  #       : serif\n",
    "    \"font.serif\": \"Computer Modern Roman\",  # : Times, Palatino, New Century Schoolbook, Bookman, Computer Modern Roman\n",
    "    \"font.size\": fsize,\n",
    "    \"legend.fontsize\": fsize,\n",
    "    \"xtick.labelsize\": fsize,\n",
    "    \"ytick.labelsize\": fsize,\n",
    "    \"axes.titlesize\": fsize,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "def set_size(width, fraction=1, subplots=(1, 1)):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Document textwidth or columnwidth in pts\n",
    "    fraction: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width * fraction\n",
    "\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5 ** 0.5 - 1) / 2\n",
    "\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    # fig_height_in = fig_width_in * golden_ratio\n",
    "    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n",
    "\n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    "\n",
    "    return fig_dim\n",
    "\n",
    "def create_figure(width=800):\n",
    "\n",
    "    return plt.subplots(figsize=set_size(width=width))\n",
    "\n",
    "\n",
    "def create_figures(nrows, ncols, width=800, tupsize=None):\n",
    "    if width:\n",
    "        return plt.subplots(nrows=nrows, ncols=ncols, figsize=set_size(width=width))\n",
    "    else:\n",
    "        return plt.subplots(nrows=nrows, ncols=ncols, figsize=tupsize)\n",
    "    \n",
    "\n",
    "def qlike(true, pred):\n",
    "    \"\"\"\n",
    "    Compute the QLIKE loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - true: numpy array of true values.\n",
    "    - pred: numpy array of predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    - QLIKE score.\n",
    "    \"\"\"\n",
    "    residuals = true - pred\n",
    "    sigma = np.std(residuals)\n",
    "    qlike = np.square(residuals / sigma)\n",
    "    return np.mean(qlike)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_node_features,\n",
    "                 num_edge_features,\n",
    "                 num_heads, \n",
    "                 output_node_channels, \n",
    "                 dim_hidden_layers=[100], \n",
    "                 dropout_att=0.0, \n",
    "                 dropout=0.0,\n",
    "                 activation='relu',\n",
    "                 concat_heads=False,\n",
    "                 negative_slope=0.2,\n",
    "                 standardize = False):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.standardize = standardize\n",
    "        \n",
    "\n",
    "        if self.standardize:\n",
    "            self.bnorm_node = nn.BatchNorm1d(num_node_features, affine=False)\n",
    "            self.bnorm_edge = nn.BatchNorm1d(num_edge_features, affine=False)\n",
    "        \n",
    "        \n",
    "        if len(dim_hidden_layers) == 1:\n",
    "            first_gat = [GATConv(in_channels=num_node_features, out_channels=dim_hidden_layers[0], heads=num_heads,  \n",
    "                                        concat=False, dropout=dropout_att, edge_dim=num_edge_features, negative_slope=negative_slope)]\n",
    "        else:\n",
    "            first_gat = [GATConv(in_channels=num_node_features, out_channels=dim_hidden_layers[0], heads=num_heads,  \n",
    "                                        concat=concat_heads, dropout=dropout_att, edge_dim=num_edge_features, negative_slope=negative_slope)]\n",
    "                     \n",
    "        stacked_gats = []\n",
    "        \n",
    "        for i in range(len(dim_hidden_layers)-1):\n",
    "            if i+1 == len(dim_hidden_layers)-1:\n",
    "                if concat_heads and num_heads>1:\n",
    "                    stacked_gats.append(GATConv(in_channels=dim_hidden_layers[i] * num_heads, \n",
    "                                                out_channels=dim_hidden_layers[i+1], heads=num_heads, \n",
    "                                                concat=False, dropout=dropout_att, edge_dim=num_edge_features, negative_slope=negative_slope))\n",
    "                else:\n",
    "                    stacked_gats.append(GATConv(in_channels=dim_hidden_layers[i], \n",
    "                                                out_channels=dim_hidden_layers[i+1], heads=num_heads, \n",
    "                                                concat=False, dropout=dropout_att, edge_dim=num_edge_features, negative_slope=negative_slope))\n",
    "            else:\n",
    "                if concat_heads and num_heads>1:\n",
    "                    stacked_gats.append(GATConv(in_channels=dim_hidden_layers[i] * num_heads, \n",
    "                                                out_channels=dim_hidden_layers[i+1], heads=num_heads, \n",
    "                                                concat=concat_heads, dropout=dropout_att, edge_dim=num_edge_features, negative_slope=negative_slope))\n",
    "                else:\n",
    "                    stacked_gats.append(GATConv(in_channels=dim_hidden_layers[i], \n",
    "                                                out_channels=dim_hidden_layers[i+1], heads=num_heads, \n",
    "                                                concat=concat_heads, dropout=dropout_att, edge_dim=num_edge_features, negative_slope=negative_slope))\n",
    "\n",
    "        \n",
    "\n",
    "        self.gat_layers = nn.ModuleList(first_gat + stacked_gats)\n",
    "\n",
    "\n",
    "        self.linear = torch.nn.Linear(dim_hidden_layers[-1], output_node_channels)\n",
    "        # Apply Xavier initialization to the linear layers\n",
    "        # torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.a = F.relu\n",
    "        elif self.activation == 'tanh':\n",
    "            self.a = F.tanh\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.a = F.sigmoid        \n",
    "        else:\n",
    "            print('Choose an available activation function')\n",
    "            sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = (data.x,\n",
    "                                    data.edge_index,\n",
    "                                    data.edge_attr)\n",
    "        self.attention_weights = []  # Initialize list to store attention weights\n",
    "        if self.standardize:\n",
    "            x = self.bnorm_node(x)\n",
    "            edge_attr = self.bnorm_node(edge_attr)\n",
    "        for l in self.gat_layers:\n",
    "            x, alpha = l(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "            self.attention_weights.append(alpha)\n",
    "            x = self.a(x)\n",
    "            if self.dropout:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x.view(-1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc82fe3-7ac4-4de9-bd00-879eb38de83c",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e3460-8743-417a-ae8e-29fafd38d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general hyperparam file\n",
    "with open('config/GNN_param.yaml', 'r') as f:\n",
    "    p = yaml.safe_load(f)\n",
    "plot_losses = p['plot_losses']\n",
    "naive_benchmark = p['naive_benchmark']\n",
    "# Define the folder path\n",
    "folder_path = 'output/{}'.format(p['model_to_load'])\n",
    "\n",
    "if 'optuna' in folder_path:\n",
    "    df = pd.read_csv('{}/study.csv'.format(folder_path), index_col=0)\n",
    "    df.set_index('number',inplace=True)\n",
    "    idx = df.sort_values('value').index[0]\n",
    "    folder_path = os.path.join(folder_path,str(idx))\n",
    "\n",
    "# folder_path = os.path.join(folder_path,str(38))\n",
    "\n",
    "# Load trained model hyperparam file\n",
    "with open('{}/GNN_param.yaml'.format(folder_path), 'r') as f:\n",
    "    p = yaml.safe_load(f)\n",
    "p['plot_losses'] = plot_losses\n",
    "p['naive_benchmark'] = naive_benchmark\n",
    "# fix randomness\n",
    "torch.manual_seed(p['seed'])\n",
    "np.random.seed(p['seed'])\n",
    "torch.cuda.manual_seed_all(p['seed'])\n",
    "\n",
    "\n",
    "# Instantiate the dataset\n",
    "if p['fully_connected']:\n",
    "    if p['output_node_channels'] == 1:\n",
    "        dataset = CovarianceLaggedDataset(hdf5_file1=p['volfile'], hdf5_file2=p['volvolfile'],root='_'.join([p['root'],str(p['seq_length'])]), seq_length=p['seq_length'])\n",
    "    else:\n",
    "        dataset = CovarianceLaggedMultiOutputDataset(hdf5_file1=p['volfile'], hdf5_file2=p['volvolfile'],root='_'.join([p['root'],str(p['seq_length']),'moutput']), seq_length=p['seq_length'], future_steps=p['output_node_channels'])\n",
    "else:\n",
    "    if p['threshold']:\n",
    "        root = '_'.join([p['root'],'sparse','t_{}'.format(p['threshold']),str(p['seq_length'])])\n",
    "    else:\n",
    "        root = '_'.join([p['root'],'sparse',str(p['seq_length'])])\n",
    "    dataset = CovarianceSparseDataset(hdf5_file=p['datafile'],root=root, seq_length=p['seq_length'], threshold=p['threshold'])\n",
    "    p['num_edge_features'] = 1\n",
    "# train-test split data\n",
    "train_size = int(p['split_proportion'] * len(dataset))\n",
    "train_dataset, test_dataset = dataset[:train_size], dataset[train_size:]\n",
    "    \n",
    "len_dataset = len(dataset)\n",
    "\n",
    "# Create DataLoaders for train and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=p['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=p['batch_size'], shuffle=False)\n",
    "\n",
    "# select dimensions from data\n",
    "NODE_FEATURES = dataset[0].x.shape[1]\n",
    "EDGE_FEATURES = dataset[0].edge_attr.shape[1]\n",
    "\n",
    "# Instantiate the model\n",
    "if p['modeltype'] == 'gat':\n",
    "    model = GATModel(num_node_features=NODE_FEATURES, \n",
    "                     num_edge_features = EDGE_FEATURES,\n",
    "                     num_heads=p['num_heads'], \n",
    "                     output_node_channels=p['output_node_channels'], \n",
    "                     dim_hidden_layers=p['dim_hidden_layers'],\n",
    "                     dropout_att = p['dropout_att'],\n",
    "                     dropout = p['dropout'],\n",
    "                     activation = p['activation'],\n",
    "                     concat_heads= p['concat_heads'],\n",
    "                     negative_slope=p['negative_slope'],\n",
    "                     standardize = p['standardize'])\n",
    "elif p['modeltype'] == 'rnn':\n",
    "    model = RecurrentGCN(num_features=p['seq_length'], \n",
    "                     hidden_channels=p['hidden_channels'], \n",
    "                     output_node_channels=p['output_node_channels'], \n",
    "                     dropout = p['dropout'],\n",
    "                     activation = p['activation'])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load saved model weights\n",
    "modelweights = os.path.join(folder_path,'{}_weights_seed_{}.pth'.format(p['modelname'], p['seed']))\n",
    "model.load_state_dict(torch.load(modelweights, map_location=device))\n",
    "\n",
    "scalers = pd.read_csv('processed_data/vols_mean_std_scalers.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6b927-4745-4a09-b76b-499590193946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b0383-5c83-46ca-8661-0139af8e4853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if p['plot_losses']:\n",
    "    train_loss = np.load('{}/train_losses_seed_{}.npy'.format(folder_path, p['seed']))\n",
    "    test_loss = np.load('{}/test_losses_seed_{}.npy'.format(folder_path, p['seed']))\n",
    "    fig2,ax2 = plt.subplots(figsize=set_size(columnwidth))\n",
    "    # ax2.plot(np.arange(len(train_loss)),np.log(train_loss), label='train')\n",
    "    # ax2.plot(np.arange(len(train_loss)),np.log(test_loss), label='test')\n",
    "    ax2.plot(np.arange(len(train_loss)),train_loss, label='train')\n",
    "    ax2.plot(np.arange(len(train_loss)),test_loss, label='test')\n",
    "    ax2.legend()\n",
    "    ax2.set_ylabel('Loss Values')\n",
    "    ax2.set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1079a-9d92-4ffa-b468-69740319ee71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p['output_node_channels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87bcb5-f4bd-48cd-9553-5c88d3cbacb5",
   "metadata": {},
   "source": [
    "# Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7bd58-dbb8-4f09-ba07-32d2291a4b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on the train set\n",
    "model.eval()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_loss = 0\n",
    "preds_train = []\n",
    "actual_train = []\n",
    "naive_benchmark = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(iterable=train_loader, desc='Testing batches...'):\n",
    "        data = data.to(device)\n",
    "        # Forward pass\n",
    "        y_x_hat = model(data) *scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        preds_train.append(y_x_hat)\n",
    "        # Compute loss\n",
    "        y_x = data.y_x *scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        actual_train.append(y_x)\n",
    "\n",
    "        naive_benchmark.append(data.x[:,0])\n",
    "\n",
    "        train_loss += criterion(y_x_hat,y_x)\n",
    "        \n",
    "# Compute average test loss\n",
    "mse = train_loss / len(train_loader)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "print('MSE',mse)\n",
    "# print('RMSE',rmse)\n",
    "\n",
    "preds_train = torch.concat(preds_train)\n",
    "actual_train = torch.concat(actual_train)\n",
    "# torch.save(preds_train, 'predictions/single_GAT_train.pt')\n",
    "# torch.save(actual_train, 'predictions/single_actual_train.pt')\n",
    "# torch.save(preds_train, 'predictions/single_GATnoedge_train.pt')\n",
    "\n",
    "reshaped_preds_train = preds_train.view(-1,30)\n",
    "reshaped_actual_train = actual_train.view(-1,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866f8ed-f651-4945-a40a-d1407349e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "test_loss = 0\n",
    "preds_val = []\n",
    "actual_val = []\n",
    "naive_benchmark = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(iterable=test_loader, desc='Testing batches...'):\n",
    "        data = data.to(device)\n",
    "        # if p['scale_up']:\n",
    "        #     data.x = data.x * p['scale_up'] \n",
    "        #     data.edge_attr = data.edge_attr * p['scale_up'] \n",
    "        #     data.y_x = data.y_x * p['scale_up']\n",
    "        # Forward pass\n",
    "        y_x_hat = model(data)*scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        preds_val.append(y_x_hat)\n",
    "        # Compute loss\n",
    "        y_x = data.y_x*scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        actual_val.append(y_x)\n",
    "\n",
    "\n",
    "        test_loss += criterion(y_x_hat,y_x)\n",
    "        \n",
    "# Compute average test loss\n",
    "mse = test_loss / len(test_loader)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "\n",
    "preds_val = torch.concat(preds_val)\n",
    "actual_val = torch.concat(actual_val)\n",
    "# torch.save(preds_val, 'predictions/single_GAT_val.pt')\n",
    "# torch.save(actual_val, 'predictions/single_actual_val.pt')\n",
    "# torch.save(preds_val, 'predictions/single_GATnoedge_val.pt')\n",
    "\n",
    "reshaped_preds_val = preds_val.view(-1,30)\n",
    "reshaped_actual_val = actual_val.view(-1,30)\n",
    "\n",
    "qlike_val = qlike(actual_val.numpy(), preds_val.numpy())\n",
    "\n",
    "# Create DataFrame\n",
    "gnn_val_metrics = pd.DataFrame({\n",
    "    'Value': [mse.numpy(), qlike_val]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "gnn_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d08da-2555-4182-89ea-216fd1ef4c05",
   "metadata": {},
   "source": [
    "## Repeat on test set, after validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897f5d7-4fe6-49e2-892f-496fd4b92dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['volvolfile'] = 'processed_data/volvols_mats_taq_standardized.h5'\n",
    "p['volfile'] = 'processed_data/vols_mats_taq_standardized.h5'\n",
    "p['root'] =  'processed_data/vols_mats_taq_standardized_test'\n",
    "\n",
    "# Instantiate the dataset\n",
    "if p['fully_connected']:\n",
    "    if p['output_node_channels'] == 1:\n",
    "        test_dataset = CovarianceLaggedDataset(hdf5_file1=p['volfile'], hdf5_file2=p['volvolfile'],root='_'.join([p['root'],str(p['seq_length'])]), seq_length=p['seq_length'])\n",
    "    else:\n",
    "        test_dataset = CovarianceLaggedMultiOutputDataset(hdf5_file1=p['volfile'], hdf5_file2=p['volvolfile'],root='_'.join([p['root'],str(p['seq_length']),'moutput']), seq_length=p['seq_length'], future_steps=p['output_node_channels'])\n",
    "else:\n",
    "    if p['threshold']:\n",
    "        root = '_'.join([p['root'],'sparse','t_{}'.format(p['threshold']),str(p['seq_length'])])\n",
    "    else:\n",
    "        root = '_'.join([p['root'],'sparse',str(p['seq_length'])])\n",
    "    test_dataset = CovarianceSparseDataset(hdf5_file=p['datafile'],root=root, seq_length=p['seq_length'], threshold=p['threshold'])\n",
    "    p['num_edge_features'] = 1\n",
    "\n",
    "len_test_dataset = len(test_dataset)\n",
    "    \n",
    "test_loader = DataLoader(test_dataset, batch_size=p['batch_size'], shuffle=False)\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "test_loss = 0\n",
    "preds_test = []\n",
    "actual_test = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(iterable=test_loader, desc='Testing batches...'):\n",
    "        data = data.to(device)\n",
    "        # Forward pass\n",
    "        y_x_hat = model(data)*scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        preds_test.append(y_x_hat)\n",
    "        # Compute loss\n",
    "        y_x = data.y_x *scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        actual_test.append(y_x)\n",
    "\n",
    "\n",
    "        test_loss += criterion(y_x_hat,y_x)\n",
    "        \n",
    "# Compute average test loss\n",
    "mse = test_loss / len(test_loader)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "\n",
    "preds_test = torch.concat(preds_test)\n",
    "actual_test = torch.concat(actual_test)\n",
    "# torch.save(preds_test, 'predictions/single_GAT_test.pt')\n",
    "# torch.save(actual_test, 'predictions/single_actual_test.pt')\n",
    "# torch.save(preds_test, 'predictions/single_GATnoedge_test.pt')\n",
    "\n",
    "reshaped_preds_test = preds_test.view(-1,30)\n",
    "reshaped_actual_test = actual_test.view(-1,30)\n",
    "\n",
    "qlike_val = qlike(actual_test.numpy(), preds_test.numpy())\n",
    "\n",
    "# Create DataFrame\n",
    "gnn_test_metrics = pd.DataFrame({\n",
    "\n",
    "    'Value': [mse.numpy(), qlike_val]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "gnn_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ef116-1a05-4db8-91cf-d80235fb0797",
   "metadata": {},
   "source": [
    "# HAR benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630719a-1d98-4676-8175-b18468229582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_to_concat = []\n",
    "for filename in tqdm(glob(os.path.join(os.getcwd(),'processed_data','vol','*.csv')),desc='Joining RVols...'):\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    # df = df.stack().reset_index(drop=True)\n",
    "    df = pd.concat([df[col] for col in df], ignore_index=True)\n",
    "    symbol = filename.split('\\\\')[-1].split('.')[0]\n",
    "    df.name = symbol\n",
    "    df_to_concat.append(df)\n",
    "\n",
    "\n",
    "vol_df = pd.concat(df_to_concat,axis=1).loc[p['seq_length'] - 14:].reset_index(drop=True)\n",
    "sorted_columns = sorted(vol_df.columns)\n",
    "vol_df = vol_df[sorted_columns]\n",
    "\n",
    "companies = list(vol_df.columns)\n",
    "\n",
    "toconcat = []\n",
    "# Loop through each column (company)\n",
    "for col in vol_df.columns:\n",
    "    temp_df = vol_df[[col]].copy()\n",
    "    temp_df['company'] = col\n",
    "    \n",
    "    # Create lagged features\n",
    "    temp_df.rename(columns={col:'RV'}, inplace=True)\n",
    "    temp_df['RV_lag_day'] = temp_df['RV'].shift(1)\n",
    "    temp_df['RV_lag_week'] = temp_df['RV'].shift(1).rolling(window = 7).mean()\n",
    "    temp_df['RV_lag_month'] = (temp_df['RV'].shift(1).rolling(window = 14).sum()-temp_df['RV'].shift(1).rolling(window = 7).sum())/ 7\n",
    "    # Drop rows with NaN (due to lag and rolling mean)\n",
    "    temp_df.dropna(inplace=True)\n",
    "    temp_df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # Append to result DataFrame\n",
    "    toconcat.append(temp_df)\n",
    "\n",
    "\n",
    "# Set index as 'company'\n",
    "har_df = pd.concat(toconcat,axis=0)\n",
    "har_df.set_index(['company', har_df.index], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26434b01-afcc-4b31-884f-9e468b7da16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists for training and testing sets\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "# Loop through each company to split its data\n",
    "for company in har_df.index.get_level_values(0).unique():\n",
    "    company_data = har_df.loc[company]\n",
    "\n",
    "    # Split the data (e.g., 80% for training, 20% for testing)\n",
    "    train_size = int(p['split_proportion'] * len_dataset)\n",
    "    train_data = company_data.iloc[:train_size]\n",
    "    val_data = company_data.iloc[train_size:len_dataset]\n",
    "    test_data = company_data.iloc[len_dataset:-238+p['seq_length']]\n",
    "    \n",
    "    # Append to the training and testing lists\n",
    "    train_list.append((company, train_data))\n",
    "    val_list.append((company, val_data))\n",
    "    test_list.append((company, test_data))\n",
    "    \n",
    "# Concatenate into DataFrames while maintaining multi-index\n",
    "train_df = pd.concat([data for company, data in train_list], keys=[company for company, _ in train_list])\n",
    "val_df = pd.concat([data for company, data in val_list], keys=[company for company, _ in val_list])\n",
    "test_df = pd.concat([data for company, data in test_list], keys=[company for company, _ in test_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e206a4-d3ee-4a7f-b46b-4f55c51738ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dependent and independent variables for training set\n",
    "Y_train = train_df['RV']\n",
    "X_train = train_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "X_train = X_train.assign(const=1)\n",
    "\n",
    "# Create and fit Panel OLS model on training set\n",
    "harmodel = PanelOLS(Y_train, X_train, entity_effects=True)\n",
    "results = harmodel.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "# Define independent variables for testing set\n",
    "X_val = val_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "X_val = X_val.assign(const=1)\n",
    "\n",
    "# Generate predictions on testing set\n",
    "predictions = results.predict(X_val)\n",
    "\n",
    "\n",
    "actual_val_har = val_df['RV'].values#/p['scale_up']\n",
    "preds_val_har = predictions.values.reshape(-1)#/p['scale_up']\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_val_har, preds_val_har)\n",
    "qlike_val = qlike(actual_val_har, preds_val_har)\n",
    "\n",
    "# Create DataFrame\n",
    "har_val_metrics = pd.DataFrame({\n",
    "    'Value': [mse, qlike_val]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "har_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af18e69-8ae8-4f12-963b-052c8b76101e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dependent and independent variables for training set\n",
    "Y_test = test_df['RV']\n",
    "X_test = test_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "X_test = X_test.assign(const=1)\n",
    "\n",
    "# Generate predictions on testing set\n",
    "predictions = results.predict(X_test)\n",
    "\n",
    "actual_test_har = test_df['RV'].values\n",
    "preds_test_har = predictions.values.reshape(-1)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_test_har, preds_test_har)\n",
    "qlike_val = qlike(actual_test_har, preds_test_har)\n",
    "\n",
    "# Create DataFrame\n",
    "har_test_metrics = pd.DataFrame({\n",
    "    'Value': [mse,  qlike_val]\n",
    "}, index = ['MSE',  'QLIKE'])\n",
    "\n",
    "har_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4abe5-84cf-432f-bc8e-15cb3d23a322",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cc1e6-c7e1-407e-905e-99bca5f4b1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
    "\n",
    "# Define target and predictors\n",
    "X_train = train_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "y_train = train_df['RV']\n",
    "\n",
    "X_val = val_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "y_val = val_df['RV']\n",
    "\n",
    "X_test = test_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "y_test = test_df['RV']\n",
    "\n",
    "# Combine the training and validation sets\n",
    "X_train_combined = np.vstack((X_train, X_val))\n",
    "y_train_combined = np.hstack((y_train, y_val))\n",
    "\n",
    "# Create the predefined split\n",
    "# -1 for training and 0 for validation\n",
    "test_fold = [-1]*len(X_train) + [0]*len(X_val)\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150, 200, 300, 400],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'gamma': [0, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "    'min_child_weight': [1, 3, 5, 7],   # Adding another parameter for good measure\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],       # L1 regularization term on weights\n",
    "    'reg_lambda': [0.5, 1, 1.5, 2]      # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "grid_search = RandomizedSearchCV(model, param_grid, cv=ps, n_iter=1000, scoring='neg_mean_squared_error', verbose=10)\n",
    "grid_search.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# Save the grid_search object after fitting\n",
    "joblib.dump(grid_search, os.path.join(folder_path, 'grid_search_results.pkl'))\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test dataset using the best model\n",
    "y_test_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e36cb-34d4-44e7-9789-337cd04f7157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = joblib.load(os.path.join('output/20231006_RGNN_layers_optuna/23', 'grid_search_results.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074403a9-50cc-4a82-8c12-f964d832a73d",
   "metadata": {},
   "source": [
    "{'subsample': 0.7, 'reg_lambda': 1.5, 'reg_alpha': 0, 'n_estimators': 400, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43ba06-3e8c-43cc-84a8-bd80099dc6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_hyperparameters = best_model.best_params_\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b80d98-3cd1-48e8-a5ed-fe7da72d5bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define target and predictors\n",
    "X_train = train_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "y_train = train_df['RV']\n",
    "\n",
    "X_val = val_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "y_val = val_df['RV']\n",
    "\n",
    "X_test = test_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']]\n",
    "y_test = test_df['RV']\n",
    "\n",
    "# Parameters from the best model\n",
    "params = {\n",
    "    'subsample': 0.7,\n",
    "    'reg_lambda': 1.5,\n",
    "    'reg_alpha': 0,\n",
    "    'n_estimators': 400,\n",
    "    'min_child_weight': 5,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.2,\n",
    "    'gamma': 0,\n",
    "    'colsample_bytree': 1,\n",
    "    'objective': 'reg:squarederror',  # Assuming regression problem\n",
    "    'tree_method': 'gpu_hist',  # Uncomment if using GPU\n",
    "    # 'gpu_id': 0  # Uncomment if using GPU\n",
    "}\n",
    "\n",
    "# Create and train the model\n",
    "best_model = xgb.XGBRegressor(**params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "actual_val_xgb = val_df['RV'].values\n",
    "preds_val_xgb = best_model.predict(X_val)\n",
    "torch.save(torch.Tensor(preds_val_xgb.reshape(30,-1).T.flatten()), 'predictions/single_xgb_val.pt')\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_val_xgb, preds_val_xgb)\n",
    "qlike_val = qlike(actual_val_xgb, preds_val_xgb)\n",
    "\n",
    "# Create DataFrame\n",
    "xgb_val_metrics = pd.DataFrame({\n",
    "    'Value': [mse,  qlike_val]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "xgb_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07482d-e8db-45bd-b976-63bf63a98548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_test_xgb = test_df['RV'].values\n",
    "preds_test_xgb = best_model.predict(X_test)\n",
    "torch.save(torch.Tensor(preds_test_xgb.reshape(30,-1).T.flatten()), 'predictions/single_xgb_test.pt')\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_test_xgb, preds_test_xgb)\n",
    "qlike_test = qlike(actual_test_xgb, preds_test_xgb)\n",
    "\n",
    "# Create DataFrame\n",
    "xgb_test_metrics = pd.DataFrame({\n",
    "    'Value': [mse, qlike_test]\n",
    "}, index = ['MSE','QLIKE'])\n",
    "\n",
    "xgb_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a2e5c-2ee1-43f2-81ee-e1ca48b1b56b",
   "metadata": {},
   "source": [
    "# LSTM benchmark\n",
    "\n",
    "The lag will become a dimension of the tensor. The features has to be all the vols, all the covols, all the volofvols and all of the covolvol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1811b2-35f4-423c-a51b-61c58f51f0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_dataset = CovarianceLSTMDataset(hdf5_file1=p['volfile'], \n",
    "                                       hdf5_file2=p['volvolfile'],\n",
    "                                       root='_'.join(['processed_data/cached_lstm_vols_mats_taq_test',str(p['seq_length'])]), \n",
    "                                       seq_length=p['seq_length'])\n",
    "lstm_dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3b3ce-ab95-4d3b-a322-35ad7df28af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "class MultivariateLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(MultivariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "root = '_'.join(['processed_data/cached_lstm_vols_mats_taq',str(p['seq_length'])])\n",
    "# Load data\n",
    "X = np.load('/'.join([root,'x_matrices.npy']))\n",
    "y = np.load('/'.join([root,'y_x_vectors.npy']))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "root = '_'.join(['processed_data/cached_lstm_vols_mats_taq_test',str(p['seq_length'])])\n",
    "# Load data\n",
    "X = np.load('/'.join([root,'x_matrices.npy']))\n",
    "y = np.load('/'.join([root,'y_x_vectors.npy']))\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "test_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Perform train-test split\n",
    "train_size = int(p['split_proportion'] * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=p['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=p['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=p['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910dc28-7428-45f2-b81c-8901edc8bf4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path_lstm = os.path.join('output','YOURPATH_TO_MODEL')\n",
    "df = pd.read_csv('{}/study.csv'.format(folder_path_lstm), index_col=0)\n",
    "df.set_index('number',inplace=True)\n",
    "idx = df.sort_values('value').index[0]\n",
    "folder_path_lstm = os.path.join(folder_path_lstm,str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f5717-fb38-4724-aa1f-17c8e225c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49070a8d-d615-4673-b420-6d5725c1efb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p['scale_up' = 10000\n",
    "input_size = train_dataset[0][0].shape[1]\n",
    "output_size = train_dataset[0][1].shape[0]\n",
    "num_layers = df.loc[idx].loc['params_num_layers']\n",
    "hidden_size = int(df.loc[idx].loc['params_hidden_size'])\n",
    "dropout = df.loc[idx].loc['params_dropout']\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lstm_model = MultivariateLSTM(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "lstm_model.load_state_dict(torch.load(os.path.join(folder_path_lstm, 'best_lstm_weights.pth'), map_location=torch.device('cpu')))\n",
    "\n",
    "lstm_model.eval()\n",
    "val_loss = 0\n",
    "\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        if p['scale_up']:\n",
    "            inputs = inputs * p['scale_up']\n",
    "            targets = targets * p['scale_up']\n",
    "        outputs = lstm_model(inputs) \n",
    "        loss = criterion(outputs/p['scale_up'], targets/p['scale_up'])\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        outputs = (outputs.cpu().numpy().flatten()/p['scale_up']) * scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        targets = (targets.cpu().numpy().flatten()/p['scale_up']) * scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        # Store the model's predictions and true values\n",
    "        predictions.extend(outputs)\n",
    "        true_values.extend(targets)\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "avg_val_rmse = math.sqrt(avg_val_loss)\n",
    "\n",
    "print(f\"Out-of-Sample val Loss: {avg_val_loss:.10f}, val RMSE: {avg_val_rmse:.10f}\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "true_values = np.array(true_values)\n",
    "torch.save(predictions, 'predictions/single_LSTM_val.pt')\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(true_values, predictions)\n",
    "qlike_test = qlike(true_values, predictions)\n",
    "\n",
    "# Create DataFrame\n",
    "lstm_val_metrics = pd.DataFrame({\n",
    "    'Value': [mse, qlike_test]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "lstm_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8962371-b05e-4ec4-aa2e-a1e031921f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        if p['scale_up']:\n",
    "            inputs = inputs * p['scale_up']\n",
    "            targets = targets * p['scale_up']\n",
    "        outputs = lstm_model(inputs) \n",
    "        loss = criterion(outputs/p['scale_up'], targets/p['scale_up'])\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        outputs = (outputs.cpu().numpy().flatten()/p['scale_up']) * scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        targets = (targets.cpu().numpy().flatten()/p['scale_up']) * scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        # Store the model's predictions and true values\n",
    "        predictions.extend(outputs)\n",
    "        true_values.extend(targets)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "avg_test_rmse = math.sqrt(avg_test_loss)\n",
    "\n",
    "print(f\"Out-of-Sample Test Loss: {avg_test_loss:.10f}, Test RMSE: {avg_test_rmse:.10f}\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "true_values = np.array(true_values)\n",
    "torch.save(predictions, 'predictions/single_LSTM_test.pt')\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(true_values, predictions)\n",
    "qlike_test = qlike(true_values, predictions)\n",
    "\n",
    "# Create DataFrame\n",
    "lstm_test_metrics = pd.DataFrame({\n",
    "    'Value': [mse, qlike_test]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "lstm_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcbe749-5cb9-4bc7-b9f4-8dc483ab9889",
   "metadata": {},
   "source": [
    "#  Multivariate HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956be8f6-5a0f-49db-931f-a03cc7c36a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "har_df.index.names = ['company', 'time']\n",
    "rv_df = har_df[['RV']].reset_index()\n",
    "regressors = har_df[['RV_lag_day', 'RV_lag_week', 'RV_lag_month']].unstack(0)\n",
    "full_regressors = pd.concat([regressors]*30).reset_index()\n",
    "mulhar_df = pd.concat([rv_df, full_regressors], axis=1)\n",
    "mulhar_df.set_index(['company','time'],inplace=True)\n",
    "new_column_names = ['_'.join(filter(None, col)) if isinstance(col, tuple) else col for col in mulhar_df.columns]\n",
    "mulhar_df.columns = new_column_names\n",
    "mulhar_df.drop('time',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c4b27-af37-4ec2-962e-0c82b757ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for training and testing sets\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "# Loop through each company to split its data\n",
    "for company in mulhar_df.index.get_level_values(0).unique():\n",
    "    company_data = mulhar_df.loc[company]\n",
    "\n",
    "    # Split the data (e.g., 80% for training, 20% for testing)\n",
    "    train_size = int(p['split_proportion'] * len_dataset)\n",
    "    train_data = company_data.iloc[:train_size]\n",
    "    val_data = company_data.iloc[train_size:len_dataset]\n",
    "    test_data = company_data.iloc[len_dataset:-238+p['seq_length']]\n",
    "    \n",
    "    # Append to the training and testing lists\n",
    "    train_list.append((company, train_data))\n",
    "    val_list.append((company, val_data))\n",
    "    test_list.append((company, test_data))\n",
    "    \n",
    "# Concatenate into DataFrames while maintaining multi-index\n",
    "train_df = pd.concat([data for company, data in train_list], keys=[company for company, _ in train_list])\n",
    "val_df = pd.concat([data for company, data in val_list], keys=[company for company, _ in val_list])\n",
    "test_df = pd.concat([data for company, data in test_list], keys=[company for company, _ in test_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c6eb8-fc14-492a-943f-05352b891221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dependent and independent variables for training set\n",
    "Y_train = train_df['RV']\n",
    "X_train = train_df[train_df.columns[1:]]\n",
    "X_train = X_train.assign(const=1)\n",
    "\n",
    "# Create and fit Panel OLS model on training set\n",
    "harmodel = PanelOLS(Y_train, X_train, entity_effects=True)\n",
    "results = harmodel.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "# Define independent variables for testing set\n",
    "X_val = val_df[val_df.columns[1:]]\n",
    "X_val = X_val.assign(const=1)\n",
    "\n",
    "# Generate predictions on testing set\n",
    "predictions = results.predict(X_val)\n",
    "torch.save(torch.Tensor(predictions.values.reshape(-1).reshape(30,-1).T.flatten()), 'predictions/single_HAR_val.pt')\n",
    "\n",
    "\n",
    "\n",
    "actual_val_har = val_df['RV'].values#/p['scale_up']\n",
    "preds_val_har = predictions.values.reshape(-1)#/p['scale_up']\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_val_har, preds_val_har)\n",
    "qlike_val = qlike(actual_val_har, preds_val_har)\n",
    "\n",
    "# Create DataFrame\n",
    "har_val_metrics = pd.DataFrame({\n",
    "    'Value': [mse, qlike_val]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "har_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df05c7a-e367-4801-839e-b1913b7776e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dependent and independent variables for training set\n",
    "Y_test = test_df['RV']\n",
    "X_test = test_df[test_df.columns[1:]]\n",
    "X_test = X_test.assign(const=1)\n",
    "\n",
    "# Generate predictions on testing set\n",
    "predictions = results.predict(X_test)\n",
    "torch.save(torch.Tensor(predictions.values.reshape(-1).reshape(30,-1).T.flatten()), 'predictions/single_HAR_test.pt')\n",
    "\n",
    "actual_test_har = test_df['RV'].values\n",
    "preds_test_har = predictions.values.reshape(-1)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_test_har, preds_test_har)\n",
    "qlike_val = qlike(actual_test_har, preds_test_har)\n",
    "\n",
    "# Create DataFrame\n",
    "har_test_metrics = pd.DataFrame({\n",
    "    'Value': [mse, qlike_val]\n",
    "}, index = ['MSE', 'QLIKE'])\n",
    "\n",
    "har_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670ba42-f708-48ca-ab2d-74967183475e",
   "metadata": {},
   "source": [
    "# PyG Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbf55a-b21e-4043-be77-650ef27c7937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer, PGExplainer, AttentionExplainer\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=p['batch_size'], shuffle=False)\n",
    "EPOCHS = 2\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        # Create a data object similar to what your original model expects\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        return self.model(data)\n",
    "\n",
    "# Wrap your original model\n",
    "wrapped_model = ModelWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c2ac6-a8c8-443e-a624-c27841343926",
   "metadata": {},
   "source": [
    "### GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f463dc-7742-4e02-9483-5b831f4a4a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ebbbf-b1d1-4e24-ba18-37ef5c519df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = Explainer(\n",
    "    model=wrapped_model,\n",
    "    algorithm=GNNExplainer(epochs=100),\n",
    "    explanation_type='phenomenon',\n",
    "    node_mask_type='object',\n",
    "    # edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='regression',\n",
    "        task_level='graph',\n",
    "        return_type='raw',\n",
    "    ),\n",
    "    threshold_config=dict(threshold_type='topk_hard', value=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f613b84-5554-4bbe-91c6-3ed5782e8b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explanation = explainer(x=train_dataset[idx].x, edge_index=train_dataset[idx].edge_index, target=train_dataset[idx].y_x,index=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18305a7c-0e01-4197-824b-f55cc1431239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "constituents = ['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS', 'DOW',\n",
    "                'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM',\n",
    "                'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT']\n",
    "dat = test_dataset[:500]\n",
    "node_count_df = pd.DataFrame(np.zeros((30, 30)), columns=range(30), index=range(30))\n",
    "for node_idx in tqdm(range(30),desc='Nodes...'):  # assuming 30 nodes\n",
    "    # Initialize the DataFrame with zeros\n",
    "    for idx in range(len(dat)):  # tqdm for progress bar\n",
    "        explanation = explainer(x=dat[idx].x, \n",
    "                                edge_index=dat[idx].edge_index, \n",
    "                                target=dat[idx].y_x, \n",
    "                                index=node_idx)\n",
    "\n",
    "        # Process the explanation\n",
    "        important_nodes = list(np.where(explanation.node_mask)[0])\n",
    "\n",
    "        # Count occurrences of each edge and update the DataFrame\n",
    "        for n in important_nodes:\n",
    "            node_count_df.loc[n, node_idx] += 1\n",
    "node_count_df.index = constituents\n",
    "node_count_df.columns = constituents\n",
    "node_count_df.to_csv('model_exp/node_counts/node_counts_test_5node.csv'.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd3f1a-7a28-4728-a252-bc22f05d69f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9192afc-9002-4560-b883-3551703e6c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('model_exp/node_counts/node_counts_val_5node.csv',index_col=0)\n",
    "# df = pd.read_csv('model_exp/node_counts/node_counts_test_5node.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d05b07-92cf-49fa-bea3-1d857e68acc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pct = df/500 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8077cc-5b81-49f0-bf55-da325a19771a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_idx = df_pct.mean(1).sort_values(ascending=False).index\n",
    "df_pct =df_pct.reindex(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2aab8-d756-4bb2-828d-c1674e9227aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "constituents = ['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS', 'DOW',\n",
    "                'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM',\n",
    "                'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT']\n",
    "cmap = LinearSegmentedColormap.from_list('custom', ['lightsteelblue', 'navy', 'gold', 'orange'], N=8)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=set_size(columnwidth*4))\n",
    "\n",
    "sns.heatmap(df_pct, cmap=cmap,yticklabels=list(new_idx),xticklabels=constituents,linecolor='gray')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.yticks(rotation=0)  # Keep y-axis labels as they are\n",
    "\n",
    "ax.set_ylabel('Source nodes', fontsize = 26)\n",
    "ax.set_xlabel('Target nodes', fontsize = 26)\n",
    "\n",
    "# fig.savefig('figs/GNNexp_val_10nodes.pdf', format='pdf', bbox_inches='tight', dpi=150)\n",
    "fig.savefig('figs/GNNexp_val_5nodes_highres.pdf', format='pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9648c-aac8-4b1b-bb91-2b06d412617a",
   "metadata": {},
   "source": [
    "# Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d0034-0b1f-4d9e-ae33-74fb1812ba27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d9ef3-4743-4134-b793-2a6d47da386f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fsize = 17\n",
    "\n",
    "# Ensure that amsmath is used in matplotlib's LaTeX\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amsmath}\"\n",
    "\n",
    "# List of three nodes to focus on\n",
    "focus_nodes = ['i', 'j', 'k']\n",
    "fig = plt.figure(figsize=set_size(columnwidth))\n",
    "# plt.xlim(plt.xlim()[0], plt.xlim()[1] + 1.0)\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the three nodes to the graph\n",
    "for node in focus_nodes:\n",
    "    G.add_node(node)\n",
    "\n",
    "# Connect all three nodes\n",
    "for i in range(len(focus_nodes)):\n",
    "    for j in range(i+1, len(focus_nodes)):\n",
    "        G.add_edge(focus_nodes[i], focus_nodes[j])\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G, seed=34)\n",
    "nx.draw(G, pos, with_labels=False, node_size=2000, node_color=\"skyblue\")\n",
    "\n",
    "# Add node names inside the nodes\n",
    "for node, (x, y) in pos.items():\n",
    "    plt.text(x, y, r\"$x_{\" + node + \"}$\", horizontalalignment='center', verticalalignment='center', fontsize=fsize, fontweight='bold')\n",
    "\n",
    "content_x_coord = max(x for _, (x, _) in pos.items()) + 0.2\n",
    "\n",
    "y_range = np.array([1.1,0.8,0.5])-0.2\n",
    "\n",
    "\n",
    "# Function to calculate a small perpendicular offset\n",
    "def perpendicular_offset(x1, y1, x2, y2, offset=2):\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    norm = np.sqrt(dx**2 + dy**2)\n",
    "    # Perpendicular direction\n",
    "    dx_perp = -dy / norm\n",
    "    dy_perp = dx / norm\n",
    "    return x1 + dx/2 + offset*dx_perp, y1 + dy/2 + offset*dy_perp\n",
    "\n",
    "# Add edge features using math notation\n",
    "for (n1, n2) in G.edges():\n",
    "    x1, y1 = pos[n1]\n",
    "    x2, y2 = pos[n2]\n",
    "\n",
    "    # Calculate offset position for the edge label\n",
    "    edge_label_x, edge_label_y = perpendicular_offset(x1, y1, x2, y2, offset=0.11)\n",
    "\n",
    "    edge_content = r\"$x^{e}_{\" + n1 + n2 + r\"}$\"\n",
    "    plt.text(edge_label_x, edge_label_y, edge_content, horizontalalignment='center', verticalalignment='center', fontsize=fsize)\n",
    "\n",
    "# Save the focused figure with a bounding box\n",
    "plt.savefig('figs/zoomgraph_no_text.pdf', format=\"pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b0afc-987d-411a-a999-afd4fb8213e8",
   "metadata": {},
   "source": [
    "# Multioutput Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d154d-5044-4119-819f-9eb619b0988c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "test_loss = 0\n",
    "preds_val = []\n",
    "actual_val = []\n",
    "naive_benchmark = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(iterable=test_loader, desc='Testing batches...'):\n",
    "        data = data.to(device)\n",
    "        # Forward pass\n",
    "        y_x_hat = model(data)*scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        preds_val.append(y_x_hat)\n",
    "        # Compute loss\n",
    "        y_x = data.y_x*scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        actual_val.append(y_x)\n",
    "\n",
    "\n",
    "        test_loss += criterion(y_x_hat,y_x)\n",
    "        \n",
    "# Compute average test loss\n",
    "mse = test_loss / len(test_loader)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "\n",
    "preds_val = torch.concat(preds_val)\n",
    "actual_val = torch.concat(actual_val)\n",
    "\n",
    "reshaped_preds_val = preds_val.view(-1, 30, 14)\n",
    "reshaped_actual_val = actual_val.view(-1, 30, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a140a75-e035-4928-b3d5-cab67193de62",
   "metadata": {},
   "source": [
    "## Repeat on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e23e73-31e7-42f5-8aaa-6fd96471e51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "p['volvolfile'] = 'processed_data/volvols_mats_taq_standardized.h5'\n",
    "p['volfile'] = 'processed_data/vols_mats_taq_standardized.h5'\n",
    "p['root'] =  'processed_data/vols_mats_taq_standardized_test'\n",
    "\n",
    "# Instantiate the dataset\n",
    "if p['fully_connected']:\n",
    "    if p['output_node_channels'] == 1:\n",
    "        test_dataset = CovarianceLaggedDataset(hdf5_file1=p['volfile'], hdf5_file2=p['volvolfile'],root='_'.join([p['root'],str(p['seq_length'])]), seq_length=p['seq_length'])\n",
    "    else:\n",
    "        test_dataset = CovarianceLaggedMultiOutputDataset(hdf5_file1=p['volfile'], hdf5_file2=p['volvolfile'],root='_'.join([p['root'],str(p['seq_length']),'moutput']), seq_length=p['seq_length'], future_steps=p['output_node_channels'])\n",
    "else:\n",
    "    if p['threshold']:\n",
    "        root = '_'.join([p['root'],'sparse','t_{}'.format(p['threshold']),str(p['seq_length'])])\n",
    "    else:\n",
    "        root = '_'.join([p['root'],'sparse',str(p['seq_length'])])\n",
    "    test_dataset = CovarianceSparseDataset(hdf5_file=p['datafile'],root=root, seq_length=p['seq_length'], threshold=p['threshold'])\n",
    "    p['num_edge_features'] = 1\n",
    "\n",
    "len_test_dataset = len(test_dataset)\n",
    "    \n",
    "test_loader = DataLoader(test_dataset, batch_size=p['batch_size'], shuffle=False)\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "test_loss = 0\n",
    "preds_test = []\n",
    "actual_test = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(iterable=test_loader, desc='Testing batches...'):\n",
    "        data = data.to(device)\n",
    "        # Forward pass\n",
    "        y_x_hat = model(data)*scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        preds_test.append(y_x_hat)\n",
    "        # Compute loss\n",
    "        y_x = data.y_x *scalers.loc['Variance','Std'] + scalers.loc['Variance','Mean']\n",
    "        actual_test.append(y_x)\n",
    "\n",
    "\n",
    "        test_loss += criterion(y_x_hat,y_x)\n",
    "        \n",
    "# Compute average test loss\n",
    "mse = test_loss / len(test_loader)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "\n",
    "preds_test = torch.concat(preds_test)\n",
    "actual_test = torch.concat(actual_test)\n",
    "\n",
    "reshaped_preds_test = preds_test.view(-1,30,14)\n",
    "reshaped_actual_test = actual_test.view(-1,30,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db6b4b-44bc-488e-bc19-49ece05e69b3",
   "metadata": {},
   "source": [
    "## Real vs Actual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2cb29-ccba-489d-bcab-5f27fa0d4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_concat = []\n",
    "for filename in tqdm(glob(os.path.join(os.getcwd(),'processed_data','vol','*.csv')),desc='Joining RVols...'):\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "\n",
    "    # df = df.stack().reset_index(drop=True)\n",
    "    df = pd.concat([df[col] for col in df], ignore_index=True)\n",
    "    symbol = filename.split('\\\\')[-1].split('.')[0]\n",
    "    df.name = symbol\n",
    "    df_to_concat.append(df)\n",
    "\n",
    "\n",
    "vol_df = pd.concat(df_to_concat,axis=1)\n",
    "# vol_df = vol_df.iloc[:len(dataset),:] #* p['scale_up']\n",
    "\n",
    "companies = list(vol_df.columns)\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "ts = pd.read_csv(os.path.join(os.getcwd(),'processed_data','timestamps.csv'), index_col=0)\n",
    "\n",
    "ts['0'] = pd.to_datetime(ts['0'])\n",
    "\n",
    "# Set the date column as the index\n",
    "ts.set_index('0', inplace=True)\n",
    "\n",
    "# Resample the data to 30-minute intervals\n",
    "ts_resampled = ts.resample('30T').mean()\n",
    "\n",
    "# Filter the data to only include times between 9:30 and 16:00\n",
    "ts_filtered = ts_resampled.between_time('09:30', '16:00')\n",
    "\n",
    "\n",
    "# Get US federal holidays\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=ts_filtered.index.min(), end=ts_filtered.index.max())\n",
    "\n",
    "# Remove rows with holiday dates\n",
    "ts_filtered_noholidays = ts_filtered[~ts_filtered.index.floor('D').isin(holidays) & ~ts_filtered.index.dayofweek.isin([5, 6])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e5aca-9080-433c-9a2d-b78d0c389206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx to use are 2, 8, 12, 21\n",
    "idxs = [2, 8, 11, 19]\n",
    "for COMPANY_IDX in idxs:\n",
    "    m = 40 #5\n",
    "    TIMESTEP = 11 + 14 * m\n",
    "\n",
    "    datetime_objs  = ts_filtered_noholidays[train_size + TIMESTEP :train_size + TIMESTEP + 14].index\n",
    "    \n",
    "    # Extract the date and time components\n",
    "    dates = [dt_obj.date() for dt_obj in datetime_objs]\n",
    "    times = [dt_obj.time() for dt_obj in datetime_objs]\n",
    "    \n",
    "    \n",
    "    # Extract predictions and actual values for the specified timestep and company\n",
    "    predictions = reshaped_preds_val[TIMESTEP+1:TIMESTEP+14+1, COMPANY_IDX].numpy()\n",
    "    actuals = reshaped_actual_val[TIMESTEP+1:TIMESTEP+14+1, COMPANY_IDX].numpy()\n",
    "    \n",
    "    # Create a range for the x-axis (number of steps)\n",
    "    steps = np.arange(len(predictions))\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=set_size(columnwidth))\n",
    "    plt.plot(steps, predictions, label='$\\widehat{V}(t)$', marker='o', color='darkblue')\n",
    "    plt.plot(steps, actuals, label='$V(t)$', marker='x', ls='--', color='gray')\n",
    "    \n",
    "    # Adding titles and labels\n",
    "    plt.title(f'{companies[COMPANY_IDX]}')\n",
    "    plt.xticks(steps, times, rotation=45);  # Adjust rotation if necessary\n",
    "    # fig.savefig('figs/multipreds_{}_on_2022-09-19.pdf'.format(companies[COMPANY_IDX],dates[0]), format='pdf', bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d3571-3cf0-4d3e-99f4-c124b623aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx to use are 2, 8, 12, 21\n",
    "idxs = [2, 8, 11, 19]\n",
    "for COMPANY_IDX in idxs:\n",
    "    m = 50 \n",
    "    TIMESTEP = 1 + 14 * m\n",
    "    datetime_objs  = ts_filtered_noholidays[len(dataset) + TIMESTEP :len(dataset) + TIMESTEP + 14].index\n",
    "    \n",
    "    # Extract the date and time components\n",
    "    dates = [dt_obj.date() for dt_obj in datetime_objs]\n",
    "    times = [dt_obj.time() for dt_obj in datetime_objs]\n",
    "    \n",
    "    \n",
    "    # Extract predictions and actual values for the specified timestep and company\n",
    "    predictions = reshaped_preds_test[TIMESTEP+1:TIMESTEP+14+1, COMPANY_IDX].numpy()\n",
    "    actuals = reshaped_actual_test[TIMESTEP+1:TIMESTEP+14+1, COMPANY_IDX].numpy()\n",
    "    \n",
    "    # Create a range for the x-axis (number of steps)\n",
    "    steps = np.arange(len(predictions))\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=set_size(columnwidth))\n",
    "    plt.plot(steps, predictions, label='$\\widehat{V}(t)$', marker='o', color='darkblue')\n",
    "    plt.plot(steps, actuals, label='$V(t)$', marker='x', ls='--', color='gray')\n",
    "    \n",
    "    # Adding titles and labels\n",
    "    plt.title(f'{companies[COMPANY_IDX]}')\n",
    "    # Setting x-ticks and x-tick labels\n",
    "    plt.xticks(steps, times, rotation=45);  # Adjust rotation if necessary\n",
    "    \n",
    "    # fig.savefig('figs/multipreds_{}_on_2023-02-01_test3.pdf'.format(companies[COMPANY_IDX],dates[0]), format='pdf', bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1fc73-f4ff-4b50-a6d0-4d0e06326fe0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a682aeca-498f-4ca6-8ee3-0ffc0ce890b4",
   "metadata": {},
   "source": [
    "# Predictions - MCS and DM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305607cb-620b-467f-8eb3-8a81fbc871b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change from single to multi when running this for multistep ahead predictions\n",
    "file_directory = os.path.join(os.getcwd(),'predictions')\n",
    "\n",
    "single_actual_val = torch.load(os.path.join(file_directory, 'single_actual_val.pt')).numpy()\n",
    "print(single_actual_val.shape,single_actual_val.mean())\n",
    "single_actual_test = torch.load(os.path.join(file_directory, 'single_actual_test.pt')).numpy()\n",
    "print(single_actual_test.shape,single_actual_test.mean())\n",
    "\n",
    "single_arf_val = torch.load(os.path.join(file_directory, 'single_arf_val_new.pt'))\n",
    "print(single_arf_val.shape,single_arf_val.mean())\n",
    "single_arf_test = torch.load(os.path.join(file_directory, 'single_arf_test_new.pt'))\n",
    "print(single_arf_test.shape,single_arf_test.mean())\n",
    "\n",
    "single_HAR_val = torch.load(os.path.join(file_directory, 'single_HAR_val_new.pt'))\n",
    "print(single_HAR_val.shape,single_HAR_val.mean())\n",
    "single_HAR_test = torch.load(os.path.join(file_directory, 'single_HAR_test_new.pt'))\n",
    "print(single_HAR_test.shape,single_HAR_test.mean())\n",
    "\n",
    "single_XGB_val = torch.load(os.path.join(file_directory, 'single_xgb_val_new.pt'))\n",
    "print(single_XGB_val.shape,single_XGB_val.mean())\n",
    "single_XGB_test = torch.load(os.path.join(file_directory, 'single_xgb_test_new.pt'))\n",
    "print(single_XGB_test.shape,single_XGB_test.mean())\n",
    "\n",
    "single_LSTM_val = torch.load(os.path.join(file_directory, 'single_lstm_val_new.pt'))\n",
    "print(single_LSTM_val.shape,single_LSTM_val.mean())\n",
    "single_LSTM_test = torch.load(os.path.join(file_directory, 'single_lstm_test_new.pt'))\n",
    "print(single_LSTM_test.shape,single_LSTM_test.mean())\n",
    "\n",
    "single_GATnoedge_val = torch.load(os.path.join(file_directory, 'single_GATnoedge_val_new.pt'))\n",
    "print(single_GATnoedge_val.shape,single_GATnoedge_val.mean())\n",
    "single_GATnoedge_test = torch.load(os.path.join(file_directory, 'single_GATnoedge_test_new.pt'))\n",
    "print(single_GATnoedge_test.shape,single_GATnoedge_test.mean())\n",
    "\n",
    "single_GAT_val = torch.load(os.path.join(file_directory, 'single_GAT_val_new.pt'))\n",
    "print(single_GAT_val.shape,single_GAT_val.mean())\n",
    "single_GAT_test = torch.load(os.path.join(file_directory, 'single_GAT_test_new.pt'))\n",
    "print(single_GAT_test.shape,single_GAT_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71ba85-9b88-407f-82ee-f8392a2f8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation')\n",
    "print('ARFIMA',mean_squared_error(single_actual_val,single_arf_val))\n",
    "print('HAR',mean_squared_error(single_actual_val,single_HAR_val))\n",
    "print('XGB',mean_squared_error(single_actual_val,single_XGB_val))\n",
    "print('LSTM',mean_squared_error(single_actual_val,single_LSTM_val))\n",
    "print('GATnoedge',mean_squared_error(single_actual_val,single_GATnoedge_val))\n",
    "print('GAT',mean_squared_error(single_actual_val,single_GAT_val))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Test')\n",
    "print('ARFIMA',mean_squared_error(single_actual_test,single_arf_test))\n",
    "print('HAR',mean_squared_error(single_actual_test,single_HAR_test))\n",
    "print('XGB',mean_squared_error(single_actual_test,single_XGB_test))\n",
    "print('LSTM',mean_squared_error(single_actual_test,single_LSTM_test))\n",
    "print('GATnoedge',mean_squared_error(single_actual_test,single_GATnoedge_test))\n",
    "print('GAT',mean_squared_error(single_actual_test,single_GAT_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbe25f-edb8-4c3d-9b0a-f7fe3154bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanqlike(y,yhat):\n",
    "    return np.mean(np.exp(y)/np.exp(yhat) - (y-yhat) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d50d85-1ee9-4fa4-8f30-caf9458b7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE')\n",
    "print('Validation')\n",
    "print('ARFIMA',mean_squared_error(single_actual_val,single_arf_val))\n",
    "print('HAR',mean_squared_error(single_actual_val,single_HAR_val))\n",
    "print('XGB',mean_squared_error(single_actual_val,single_XGB_val))\n",
    "print('LSTM',mean_squared_error(single_actual_val,single_LSTM_val))\n",
    "print('GATnoedge',mean_squared_error(single_actual_val,single_GATnoedge_val))\n",
    "print('GAT',mean_squared_error(single_actual_val,single_GAT_val))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Test')\n",
    "print('ARFIMA',mean_squared_error(single_actual_test,single_arf_test))\n",
    "print('HAR',mean_squared_error(single_actual_test,single_HAR_test))\n",
    "print('XGB',mean_squared_error(single_actual_test,single_XGB_test))\n",
    "print('LSTM',mean_squared_error(single_actual_test,single_LSTM_test))\n",
    "print('GATnoedge',mean_squared_error(single_actual_test,single_GATnoedge_test))\n",
    "print('GAT',mean_squared_error(single_actual_test,single_GAT_test))\n",
    "\n",
    "print()\n",
    "print('QLIKE')\n",
    "print('Validation')\n",
    "print('ARFIMA',meanqlike(single_actual_val,single_arf_val))\n",
    "print('HAR',meanqlike(single_actual_val,single_HAR_val))\n",
    "print('XGB',meanqlike(single_actual_val,single_XGB_val))\n",
    "print('LSTM',meanqlike(single_actual_val,single_LSTM_val))\n",
    "print('GATnoedge',meanqlike(single_actual_val,single_GATnoedge_val))\n",
    "print('GAT',meanqlike(single_actual_val,single_GAT_val))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Test')\n",
    "print('ARFIMA',meanqlike(single_actual_test,single_arf_test))\n",
    "print('HAR',meanqlike(single_actual_test,single_HAR_test))\n",
    "print('XGB',meanqlike(single_actual_test,single_XGB_test))\n",
    "print('LSTM',meanqlike(single_actual_test,single_LSTM_test))\n",
    "print('GATnoedge',meanqlike(single_actual_test,single_GATnoedge_test))\n",
    "print('GAT',meanqlike(single_actual_test,single_GAT_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438dd82d-5077-4a04-8138-115ad83e052d",
   "metadata": {},
   "source": [
    "## MCS\n",
    "\n",
    "https://arch.readthedocs.io/en/stable/multiple-comparison/multiple-comparison_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3413056-445f-4845-9e52-3ad842e68d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.bootstrap import MCS\n",
    "def qlike(y,yhat):\n",
    "    return (np.log(yhat) + (y/yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e711581-2bd5-46b0-9ec3-ed18ebc7b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE on VAl\n",
    "models = ['ARFIMA', 'HAR', 'XGB', 'LSTM', 'SPOtV2NetNoedge', 'SpotV2Net']\n",
    "vals = [single_arf_val, single_HAR_val, single_XGB_val, single_LSTM_val, \n",
    "        single_GATnoedge_val, single_GAT_val]\n",
    "\n",
    "\n",
    "losses = pd.DataFrame(columns=models,\n",
    "                      index=range(len(single_actual_val)),\n",
    "                      data=np.array([(single_actual_val-preds)**2 for preds in vals]).transpose())\n",
    "\n",
    "mcs = MCS(losses, size=0.05)\n",
    "mcs.compute()\n",
    "print(\"MCS P-values\")\n",
    "print(mcs.pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dcf624-8a71-4497-97f3-f0f142527886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLIKE on VAl\n",
    "models = ['ARFIMA', 'HAR', 'XGB', 'LSTM', 'SPOtV2NetNoedge', 'SpotV2Net']\n",
    "vals = [single_arf_val, single_HAR_val, single_XGB_val, single_LSTM_val, \n",
    "        single_GATnoedge_val, single_GAT_val]\n",
    "\n",
    "\n",
    "losses = pd.DataFrame(columns=models,\n",
    "                      index=range(len(single_actual_val)),\n",
    "                      data=np.array([qlike(single_actual_val,preds) for preds in vals]).transpose())\n",
    "\n",
    "mcs = MCS(losses, size=0.05)\n",
    "mcs.compute()\n",
    "print(\"MCS P-values\")\n",
    "print(mcs.pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ac978-4aab-4ce7-8e9d-b1f2ba677637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE on TEST\n",
    "models = ['ARFIMA', 'HAR', 'XGB', 'LSTM', 'SPOtV2NetNoedge', 'SpotV2Net']\n",
    "tests = [single_arf_test, single_HAR_test, single_XGB_test, single_LSTM_test, \n",
    "         single_GATnoedge_test, single_GAT_test]\n",
    "\n",
    "\n",
    "losses = pd.DataFrame(columns=models,\n",
    "                      index=range(len(single_actual_test)),\n",
    "                      data=np.array([(single_actual_test-preds)**2 for preds in tests]).transpose())\n",
    "\n",
    "mcs = MCS(losses, size=0.05)\n",
    "mcs.compute()\n",
    "print(\"MCS P-values\")\n",
    "print(mcs.pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04024f70-4160-4db3-93ff-4cbe7528987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLIKE on TEST\n",
    "models = ['ARFIMA', 'HAR', 'XGB', 'LSTM', 'SPOtV2NetNoedge', 'SpotV2Net']\n",
    "tests = [single_arf_test, single_HAR_test, single_XGB_test, single_LSTM_test, \n",
    "         single_GATnoedge_test, single_GAT_test]\n",
    "\n",
    "\n",
    "losses = pd.DataFrame(columns=models,\n",
    "                      index=range(len(single_actual_test)),\n",
    "                      data=np.array([qlike(single_actual_test,preds) for preds in tests]).transpose())\n",
    "\n",
    "mcs = MCS(losses, size=0.05)\n",
    "mcs.compute()\n",
    "print(\"MCS P-values\")\n",
    "print(mcs.pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef3001-7a79-49b0-a4cb-85cc9e9b18f3",
   "metadata": {},
   "source": [
    "## DM adaptation from EPFToolbox to include QLIKE\n",
    "\n",
    "https://github.com/jeslago/epftoolbox/blob/master/epftoolbox/evaluation/_dm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747741a6-81f8-4669-b106-7f591e8bc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from itertools import combinations\n",
    "\n",
    "def meanqlike(y,yhat):\n",
    "    return np.mean(y/yhat - np.log(y/yhat) - 1)\n",
    "\n",
    "def DM(p_real, p_pred_1, p_pred_2, norm=1, version='univariate'):\n",
    "    \"\"\"Function that performs the one-sided DM test in the contex of electricity price forecasting\n",
    "    \n",
    "    The test compares whether there is a difference in predictive accuracy between two forecast \n",
    "    ``p_pred_1`` and ``p_pred_2``. Particularly, the one-sided DM test evaluates the null hypothesis H0 \n",
    "    of the forecasting errors of  ``p_pred_2`` being larger (worse) than the forecasting\n",
    "    errors ``p_pred_1`` vs the alternative hypothesis H1 of the errors of ``p_pred_2`` being smaller (better).\n",
    "    Hence, rejecting H0 means that the forecast ``p_pred_2`` is significantly more accurate\n",
    "    that forecast ``p_pred_1``. (Note that this is an informal definition. For a formal one we refer to \n",
    "    `here <https://epftoolbox.readthedocs.io/en/latest/modules/cite.html>`_)\n",
    "\n",
    "    Two versions of the test are possible:\n",
    "\n",
    "        1. A univariate version with as many independent tests performed as prices per day, i.e. 24\n",
    "        tests in most day-ahead electricity markets.\n",
    "\n",
    "        2. A multivariate with the test performed jointly for all hours using the multivariate \n",
    "        loss differential series (see this \n",
    "        `article <https://epftoolbox.readthedocs.io/en/latest/modules/cite.html>`_ for details.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_real : numpy.ndarray\n",
    "        Array of shape :math:`(n_\\\\mathrm{days}, n_\\\\mathrm{prices/day})` representing the real market\n",
    "        prices\n",
    "    p_pred_1 : TYPE\n",
    "        Array of shape :math:`(n_\\\\mathrm{days}, n_\\\\mathrm{prices/day})` representing the first forecast\n",
    "    p_pred_2 : TYPE\n",
    "        Array of shape :math:`(n_\\\\mathrm{days}, n_\\\\mathrm{prices/day})` representing the second forecast\n",
    "    norm : int, optional\n",
    "        Norm used to compute the loss differential series. At the moment, this value must either\n",
    "        be 1 (for the norm-1) or 2 (for the norm-2).\n",
    "    version : str, optional\n",
    "        Version of the test as defined in \n",
    "        `here <https://epftoolbox.readthedocs.io/en/latest/modules/cite.html>`_. It can have two values:\n",
    "        ``'univariate`` or ``'multivariate``      \n",
    "    Returns\n",
    "    -------\n",
    "    float, numpy.ndarray\n",
    "        The p-value after performing the test. It is a float in the case of the multivariate test\n",
    "        and a numpy array with a p-value per hour for the univariate test\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from epftoolbox.evaluation import DM\n",
    "    >>> from epftoolbox.data import read_data\n",
    "    >>> import pandas as pd\n",
    "    >>> \n",
    "    >>> # Generating forecasts of multiple models\n",
    "    >>> \n",
    "    >>> # Download available forecast of the NP market available in the library repository\n",
    "    >>> # These forecasts accompany the original paper\n",
    "    >>> forecasts = pd.read_csv('https://raw.githubusercontent.com/jeslago/epftoolbox/master/' + \n",
    "    ...                       'forecasts/Forecasts_NP_DNN_LEAR_ensembles.csv', index_col=0)\n",
    "    >>> \n",
    "    >>> # Deleting the real price field as it the actual real price and not a forecast\n",
    "    >>> del forecasts['Real price']\n",
    "    >>> \n",
    "    >>> # Transforming indices to datetime format\n",
    "    >>> forecasts.index = pd.to_datetime(forecasts.index)\n",
    "    >>> \n",
    "    >>> # Extracting the real prices from the market\n",
    "    >>> _, df_test = read_data(path='.', dataset='NP', begin_test_date=forecasts.index[0], \n",
    "    ...                        end_test_date=forecasts.index[-1])\n",
    "    Test datasets: 2016-12-27 00:00:00 - 2018-12-24 23:00:00\n",
    "    >>> \n",
    "    >>> real_price = df_test.loc[:, ['Price']]\n",
    "    >>> \n",
    "    >>> # Testing the univariate DM version on an ensemble of DNN models versus an ensemble\n",
    "    >>> # of LEAR models\n",
    "    >>> DM(p_real=real_price.values.reshape(-1, 24), \n",
    "    ...     p_pred_1=forecasts.loc[:, 'LEAR Ensemble'].values.reshape(-1, 24), \n",
    "    ...     p_pred_2=forecasts.loc[:, 'DNN Ensemble'].values.reshape(-1, 24), \n",
    "    ...     norm=1, version='univariate')\n",
    "    array([9.99999944e-01, 9.97562415e-01, 8.10333949e-01, 8.85201928e-01,\n",
    "           9.33505978e-01, 8.78116764e-01, 1.70135981e-02, 2.37961920e-04,\n",
    "           5.52337353e-04, 6.07843340e-05, 1.51249750e-03, 1.70415008e-03,\n",
    "           4.22319907e-03, 2.32808010e-03, 3.55958698e-03, 4.80663621e-03,\n",
    "           1.64841032e-04, 4.55829140e-02, 5.86609688e-02, 1.98878375e-03,\n",
    "           1.04045731e-01, 8.71203187e-02, 2.64266732e-01, 4.06676195e-02])\n",
    "    >>> \n",
    "    >>> # Testing the multivariate DM version\n",
    "    >>> DM(p_real=real_price.values.reshape(-1, 24), \n",
    "    ...     p_pred_1=forecasts.loc[:, 'LEAR Ensemble'].values.reshape(-1, 24), \n",
    "    ...     p_pred_2=forecasts.loc[:, 'DNN Ensemble'].values.reshape(-1, 24), \n",
    "    ...     norm=1, version='multivariate')\n",
    "    0.003005725748326471\n",
    "    \"\"\"\n",
    "\n",
    "    # Checking that all time series have the same shape\n",
    "    if p_real.shape != p_pred_1.shape or p_real.shape != p_pred_2.shape:\n",
    "        raise ValueError('The three time series must have the same shape')\n",
    "\n",
    "    # Ensuring that time series have shape (n_days, n_prices_day)\n",
    "    if len(p_real.shape) == 1 or (len(p_real.shape) == 2 and p_real.shape[1] == 1):\n",
    "        raise ValueError('The time series must have shape (n_days, n_prices_day')\n",
    "\n",
    "    # Computing the errors of each forecast\n",
    "    errors_pred_1 = p_real - p_pred_1\n",
    "    errors_pred_2 = p_real - p_pred_2\n",
    "\n",
    "\n",
    "    # Computing the test statistic\n",
    "    if version == 'univariate':\n",
    "\n",
    "        # Computing the loss differential series for the univariate test\n",
    "        if norm == 1:\n",
    "            d = np.abs(errors_pred_1) - np.abs(errors_pred_2)\n",
    "        if norm == 2:\n",
    "            d = errors_pred_1**2 - errors_pred_2**2\n",
    "\n",
    "        # Computing the loss differential size\n",
    "        N = d.shape[0]\n",
    "\n",
    "        # Computing the test statistic\n",
    "        mean_d = np.mean(d, axis=0)\n",
    "        var_d = np.var(d, ddof=0, axis=0)\n",
    "        DM_stat = mean_d / np.sqrt((1 / N) * var_d)\n",
    "\n",
    "    elif version == 'multivariate':\n",
    "\n",
    "        # Computing the loss differential series for the multivariate test\n",
    "        if norm == 1:\n",
    "            d = np.mean(np.abs(errors_pred_1), axis=1) - np.mean(np.abs(errors_pred_2), axis=1)\n",
    "        if norm == 2:\n",
    "            d = np.mean(errors_pred_1**2, axis=1) - np.mean(errors_pred_2**2, axis=1)\n",
    "        if norm == 3: #qlike\n",
    "            d = (np.mean(p_real/p_pred_1 - np.log(p_real/p_pred_1) - 1, axis=1) - \n",
    "                 np.mean(p_real/p_pred_2 - np.log(p_real/p_pred_2) - 1, axis=1))\n",
    "            \n",
    "\n",
    "        # Computing the loss differential size\n",
    "        N = d.size\n",
    "\n",
    "        # Computing the test statistic\n",
    "        mean_d = np.mean(d)\n",
    "        var_d = np.var(d, ddof=0)\n",
    "        # print(d.shape,mean_d,var_d)\n",
    "        DM_stat = mean_d / np.sqrt((1 / N) * var_d)\n",
    "        \n",
    "    p_value = 1 - stats.norm.cdf(DM_stat)\n",
    "\n",
    "    return DM_stat,p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2caa50-7196-4bd8-97c4-63856c84e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change from single to multi to perform the same for multi-step models\n",
    "models = ['ARFIMA', 'HAR', 'XGB', 'LSTM', 'SPOtV2NetNoedge', 'SpotV2Net']\n",
    "vals = [single_arf_val, single_HAR_val, single_XGB_val, single_LSTM_val, single_GATnoedge_val, single_GAT_val]\n",
    "tests = [single_arf_test, single_HAR_test, single_XGB_test, single_LSTM_test, single_GATnoedge_test, single_GAT_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec45978-ee51-496c-85c0-79d52d80ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmdf_mse_val = pd.DataFrame(index=models, columns=models)\n",
    "y = single_actual_val\n",
    "# Iterate over each combination of models and evaluate\n",
    "for (i, model1), (j, model2) in combinations(enumerate(models), 2):\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   vals[i].reshape(-1,30),\n",
    "                   vals[j].reshape(-1,30), \n",
    "                   norm=2,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_mse_val.at[model1, model2] = (np.round(S,2), np.round(pval,2))\n",
    "\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   vals[j].reshape(-1,30),\n",
    "                   vals[i].reshape(-1,30), \n",
    "                   norm=2,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_mse_val.at[model2, model1] = (np.round(S,2), np.round(pval,2))\n",
    "    # print(model2,model1)\n",
    "    # print(S,pval)\n",
    "print('DM on validation using MSE')\n",
    "dmdf_mse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743bf70-4395-45af-8597-1cdcf70f4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmdf_qlike_val = pd.DataFrame(index=models, columns=models)\n",
    "y = single_actual_val\n",
    "# Iterate over each combination of models and evaluate\n",
    "for (i, model1), (j, model2) in combinations(enumerate(models), 2):\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   vals[i].reshape(-1,30),\n",
    "                   vals[j].reshape(-1,30), \n",
    "                   norm=3,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_qlike_val.at[model1, model2] = (np.round(S,2), np.round(pval,2))\n",
    "\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   vals[j].reshape(-1,30),\n",
    "                   vals[i].reshape(-1,30), \n",
    "                   norm=3,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_qlike_val.at[model2, model1] = (np.round(S,2), np.round(pval,2))\n",
    "print('DM on validation using QLIKE')\n",
    "dmdf_qlike_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf07ee-25a6-4d44-83bd-77e0e991419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmdf_mse_test = pd.DataFrame(index=models, columns=models)\n",
    "y = single_actual_test\n",
    "# Iterate over each combination of models and evaluate\n",
    "for (i, model1), (j, model2) in combinations(enumerate(models), 2):\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   tests[i].reshape(-1,30),\n",
    "                   tests[j].reshape(-1,30), \n",
    "                   norm=2,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_mse_test.at[model1, model2] = (np.round(S,2), np.round(pval,2))\n",
    "\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   tests[j].reshape(-1,30),\n",
    "                   tests[i].reshape(-1,30), \n",
    "                   norm=2,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_mse_test.at[model2, model1] = (np.round(S,2), np.round(pval,2))\n",
    "print('DM on test using MSE')\n",
    "dmdf_mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52fe24-5a98-4c42-ad53-a31e55a7adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmdf_qlike_test = pd.DataFrame(index=models, columns=models)\n",
    "y = single_actual_test\n",
    "# Iterate over each combination of models and evaluate\n",
    "for (i, model1), (j, model2) in combinations(enumerate(models), 2):\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   tests[i].reshape(-1,30),\n",
    "                   tests[j].reshape(-1,30), \n",
    "                   norm=3,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_qlike_test.at[model1, model2] = (np.round(S,2), np.round(pval,2))\n",
    "\n",
    "    S, pval = DM(y.reshape(-1,30), \n",
    "                   tests[j].reshape(-1,30),\n",
    "                   tests[i].reshape(-1,30), \n",
    "                   norm=3,\n",
    "                  version='multivariate')\n",
    "    # Store the results in the DataFrame\n",
    "    dmdf_qlike_test.at[model2, model1] = (np.round(S,2), np.round(pval,2))\n",
    "print('DM on test using QLIKE')\n",
    "dmdf_qlike_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34793636-d64b-4975-b54c-e6117de005e5",
   "metadata": {},
   "source": [
    "# Figure Motivation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cbf3a0-b82d-46f7-b44d-990b91716d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb5a73-3f41-4b10-9eb6-e1fddb9da230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_concat = []\n",
    "for filename in tqdm(glob(os.path.join(os.getcwd(),'processed_data','vol','*.csv')),desc='Joining RVols...'):\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "\n",
    "    # df = df.stack().reset_index(drop=True)\n",
    "    df = pd.concat([df[col] for col in df], ignore_index=True)\n",
    "    symbol = filename.split('\\\\')[-1].split('.')[0]\n",
    "    df.name = symbol\n",
    "    df_to_concat.append(df)\n",
    "\n",
    "\n",
    "vol_df = pd.concat(df_to_concat,axis=1)\n",
    "sorted_columns = sorted(vol_df.columns)\n",
    "vol_df = vol_df[sorted_columns]\n",
    "vol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a622e-6468-4d3c-85ea-9591f0ee6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df.iloc[1].values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d21365-39f8-4d84-afe1-40c237eceb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df[['AXP_V','CAT_V','AXP_VZ','CAT_VZ']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8630942c-b0b2-4445-a92d-748224dfa8a2",
   "metadata": {},
   "source": [
    "# Covols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e89906-07e1-417d-b0af-39575b279768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02983d-44dc-4368-9ea5-b48cc7d5930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "ts = pd.read_csv(os.path.join(os.getcwd(),'processed_data','timestamps.csv'), index_col=0)\n",
    "\n",
    "ts['0'] = pd.to_datetime(ts['0'])\n",
    "\n",
    "# Set the date column as the index\n",
    "ts.set_index('0', inplace=True)\n",
    "\n",
    "# Resample the data to 30-minute intervals\n",
    "ts_resampled = ts.resample('30T').mean()\n",
    "\n",
    "# Filter the data to only include times between 9:30 and 16:00\n",
    "ts_filtered = ts_resampled.between_time('09:30', '16:00')\n",
    "\n",
    "\n",
    "# Get US federal holidays\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=ts_filtered.index.min(), end=ts_filtered.index.max())\n",
    "\n",
    "# Remove rows with holiday dates\n",
    "ts_filtered_noholidays = ts_filtered[~ts_filtered.index.floor('D').isin(holidays) & ~ts_filtered.index.dayofweek.isin([5, 6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f72742-e577-48da-bdab-b289125ffbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_concat = []\n",
    "for filename in tqdm(glob(os.path.join(os.getcwd(),'processed_data','small_subset_vol','*.csv')),desc='Joining RVols...'):\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    df = pd.concat([df[col] for col in df], ignore_index=True)\n",
    "    symbol = filename.split('\\\\')[-1].split('.')[0]\n",
    "    df.name = symbol\n",
    "    df_to_concat.append(df)\n",
    "\n",
    "vol_df = pd.concat(df_to_concat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f8ca2-0717-4ebc-a6b9-61f5fa503994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same to save covolvol\n",
    "vol_df.loc[:10318-1].set_index(ts_filtered_noholidays.iloc[:10318].index).loc['2023-03'].to_csv('estimated_covol_Mar23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359250a-90b5-49c8-9432-f6a8dc16f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = pd.read_csv('estimated_covol_Mar23.csv')\n",
    "vol['date'] = pd.to_datetime(vol['0']).dt.date\n",
    "vol['date'] = vol['date'].astype(str)\n",
    "vol.set_index('date',inplace=True)\n",
    "covol = pd.read_csv('estimated_covolvol_Mar23.csv')\n",
    "covol['date'] = pd.to_datetime(covol['0']).dt.date\n",
    "covol['date'] = covol['date'].astype(str)\n",
    "covol.set_index('date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20698ef-9033-4a52-9949-c2d38e2cf966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator,ScalarFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487da46-c7b4-4c19-b097-a9a73e2ca087",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_xticks = 7\n",
    "num_yticks = 7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=set_size(columnwidth))\n",
    "\n",
    "# Plot the data\n",
    "vol[['AXP', 'CAT', 'V']].plot(ax=ax, lw=1.0)\n",
    "\n",
    "\n",
    "# Ensure tick positions are within the bounds of the DataFrame\n",
    "tick_positions = [i * (len(vol) - 1) // (num_xticks - 1) for i in range(num_xticks)]\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels([vol.index[i] for i in tick_positions], rotation=45)\n",
    "\n",
    "# Hide the x-axis label\n",
    "ax.set_xlabel('')\n",
    "\n",
    "# Set scientific notation on the y-axis\n",
    "formatter = ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-3, 3))\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Increase the number of y-axis ticks\n",
    "ax.yaxis.set_major_locator(MaxNLocator(nbins=num_yticks))\n",
    "ax.set_xlim([0, len(vol) - 1])\n",
    "# Make the legend smaller in font size\n",
    "ax.legend(fontsize='small')\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "fig.savefig('figs/vol_march23_fixed.pdf', format='pdf', bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3a53f-1ea7-4062-9ef9-f5a4793709e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_xticks = 7\n",
    "num_yticks = 7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=set_size(columnwidth))\n",
    "\n",
    "# Plot the data\n",
    "covol.columns = covol.columns.str.replace('_', '-')\n",
    "covol[['AXP-V', 'CAT-V', 'AXP-CAT']].plot(ax=ax, lw=1.0)\n",
    "\n",
    "# covolplot = covol[['AXP-V', 'CAT-V', 'AXP-CAT']][maskcovol.values]\n",
    "# covolplot.plot(ax=ax, lw=1.0)\n",
    "\n",
    "# Ensure tick positions are within the bounds of the DataFrame\n",
    "tick_positions = [i * (len(covol) - 1) // (num_xticks - 1) for i in range(num_xticks)]\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels([vol.index[i] for i in tick_positions], rotation=45)\n",
    "\n",
    "# Hide the x-axis label\n",
    "ax.set_xlabel('')\n",
    "\n",
    "# Set scientific notation on the y-axis\n",
    "formatter = ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-3, 3))\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Increase the number of y-axis ticks\n",
    "ax.yaxis.set_major_locator(MaxNLocator(nbins=num_yticks))\n",
    "ax.set_xlim([0, len(covol) - 1])\n",
    "# Make the legend smaller in font size\n",
    "ax.legend(fontsize='small')\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "fig.savefig('figs/covolvol_march23_fixed.pdf', format='pdf', bbox_inches='tight', dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
